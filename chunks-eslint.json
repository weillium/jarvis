[{"filePath":"/Users/will-liao/Desktop/Coding/Git/jarvis/worker/context/pipeline/chunks-builder.ts","messages":[{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'parseError' is defined but never used.","line":542,"column":14,"nodeType":null,"messageId":"unusedVar","endLine":542,"endColumn":24},{"ruleId":"@typescript-eslint/require-await","severity":2,"message":"Async function 'rankChunks' has no 'await' expression.","line":578,"column":1,"nodeType":"FunctionDeclaration","messageId":"missingAwait","endLine":578,"endColumn":26,"suggestions":[{"messageId":"removeAsync","fix":{"range":[20798,20940],"text":"function rankChunks(\n  chunks: ChunkWithRank[],\n  blueprint: Blueprint,\n  openai: OpenAI,\n  genModel: string\n): ChunkWithRank[]"},"desc":"Remove 'async'."}]},{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'blueprint' is defined but never used.","line":580,"column":3,"nodeType":null,"messageId":"unusedVar","endLine":580,"endColumn":12},{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'openai' is defined but never used.","line":581,"column":3,"nodeType":null,"messageId":"unusedVar","endLine":581,"endColumn":9},{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'genModel' is defined but never used.","line":582,"column":3,"nodeType":null,"messageId":"unusedVar","endLine":582,"endColumn":11},{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'index' is defined but never used.","line":594,"column":43,"nodeType":null,"messageId":"unusedVar","endLine":594,"endColumn":48}],"suppressedMessages":[],"errorCount":6,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"/**\n * Enhanced Chunks Builder\n * Builds ranked context chunks from research results, documents, and LLM generation\n * Stores chunks in context_items table with rank and research_source\n */\n\nimport type { createClient } from '@supabase/supabase-js';\nimport type OpenAI from 'openai';\nimport type { Blueprint } from './blueprint-generator';\nimport type { ResearchResults, ResearchChunkMetadata } from './glossary-builder';\nimport {\n  CONTEXT_CHUNKS_GENERATION_SYSTEM_PROMPT,\n  createContextChunksUserPrompt,\n} from '../../prompts';\nimport {\n  calculateOpenAICost,\n  getPricingVersion,\n} from './pricing-config';\nimport type { OpenAIUsage } from './pricing-config';\n\nexport interface ChunksBuilderOptions {\n  supabase: ReturnType<typeof createClient>;\n  openai: OpenAI;\n  embedModel: string;\n  genModel: string;\n}\n\nexport interface ChunkWithRank {\n  text: string;\n  source: string;\n  research_source: string;\n  rank: number;\n  quality_score?: number;\n  metadata?: ChunkMetadata;\n}\n\ntype ChunkMetadata = ResearchChunkMetadata;\ntype ResearchChunk = ResearchResults['chunks'][number];\n\n/**\n * Build context chunks from blueprint plan, research results, and documents\n * Ranks chunks and stores top N in context_items table\n * Fetches research from research_results table if not provided\n */\nexport interface ChunksCostBreakdown {\n  openai: {\n    total: number;\n    chat_completions: Array<{ cost: number; usage: OpenAIUsage; model: string }>;\n    embeddings: Array<{ cost: number; usage: OpenAIUsage; model: string }>;\n  };\n}\n\nexport interface ChunksBuildResult {\n  chunkCount: number;\n  costBreakdown: ChunksCostBreakdown;\n}\n\ntype SupabaseErrorLike = { message: string } | null;\ntype SupabaseMutationResult = { error: SupabaseErrorLike };\ntype SupabaseListResult<T> = { data: T[] | null; error: SupabaseErrorLike };\ntype IdRow = { id: string };\ntype ResearchResultRecord = {\n  content: string;\n  metadata: ChunkMetadata | null;\n  query: string | null;\n  api: string | null;\n};\ntype ChatCompletionRequest = Parameters<OpenAI['chat']['completions']['create']>[0];\nconst asDbPayload = <T>(payload: T) => payload as unknown as never;\n\nexport async function buildContextChunks(\n  eventId: string,\n  blueprintId: string,\n  generationCycleId: string,\n  blueprint: Blueprint,\n  researchResults: ResearchResults | null,\n  options: ChunksBuilderOptions\n): Promise<ChunksBuildResult> {\n  const { supabase, openai, embedModel, genModel } = options;\n\n  console.log(`[chunks] Building context chunks for event ${eventId}, cycle ${generationCycleId}`);\n  console.log(`[chunks] Target: ${blueprint.chunks_plan.target_count} chunks (${blueprint.chunks_plan.quality_tier} tier)`);\n\n  // Initialize cost tracking\n  const costBreakdown: ChunksCostBreakdown = {\n    openai: {\n      total: 0,\n      chat_completions: [],\n      embeddings: [],\n    },\n  };\n\n  // Fetch research from research_results table if not provided\n  // Exclude research from superseded generation cycles\n  let research: ResearchResults;\n  if (!researchResults) {\n    // First, get all active (non-superseded) generation cycle IDs for research\n    const {\n      data: activeCycles,\n      error: cycleError,\n    }: SupabaseListResult<IdRow> = await supabase\n      .from('generation_cycles')\n      .select('id')\n      .eq('event_id', eventId)\n      .neq('status', 'superseded')\n      .in('cycle_type', ['research']);\n\n    if (cycleError) {\n      console.warn(`[chunks] Warning: Failed to fetch active research cycles: ${cycleError.message}`);\n    }\n\n    // Build list of active cycle IDs\n    const activeCycleIds: string[] = [];\n    if (activeCycles && activeCycles.length > 0) {\n      activeCycleIds.push(...activeCycles.map(c => c.id));\n    }\n\n    // Fetch research results only from active cycles (or legacy items)\n    let researchQuery = supabase\n      .from('research_results')\n      .select('content, metadata, query, api')\n      .eq('event_id', eventId)\n      .eq('blueprint_id', blueprintId);\n\n    if (activeCycleIds.length > 0) {\n      // Include items with null generation_cycle_id OR items from active cycles\n      researchQuery = researchQuery.or(`generation_cycle_id.is.null,generation_cycle_id.in.(${activeCycleIds.join(',')})`);\n    } else {\n      // If no active cycles, only show legacy items (null generation_cycle_id)\n      researchQuery = researchQuery.is('generation_cycle_id', null);\n    }\n\n    const {\n      data: researchData,\n      error: researchError,\n    }: SupabaseListResult<ResearchResultRecord> = await researchQuery;\n\n    if (researchError) {\n      console.warn(`[chunks] Warning: Failed to fetch research results: ${researchError.message}`);\n    }\n\n    research = {\n      chunks: (researchData ?? []).map(item => ({\n        text: item.content,\n        source: item.api || 'research',\n        metadata: item.metadata || undefined,\n      })),\n    };\n  } else {\n    research = researchResults;\n  }\n\n  // Legacy deletion code removed - we now use superseding approach\n  // Old chunks are marked as superseded via generation cycles, not deleted\n\n  // Update generation cycle to processing\n  const { error: processingError }: SupabaseMutationResult = await supabase\n    .from('generation_cycles')\n    .update(asDbPayload({\n      status: 'processing',\n      progress_total: blueprint.chunks_plan.target_count || 500,\n    }))\n    .eq('id', generationCycleId);\n\n  if (processingError) {\n    console.warn(`[chunks] Failed to mark generation cycle as processing: ${processingError.message}`);\n  }\n\n  // 1. Collect chunks from all sources\n  const allChunks: ChunkWithRank[] = [];\n\n  // Add research result chunks\n  for (const chunk of research.chunks) {\n    // Validate chunk text before adding\n    if (!chunk.text || typeof chunk.text !== 'string' || chunk.text.trim().length === 0) {\n      console.warn(`[chunks] Skipping research chunk with invalid text: ${typeof chunk.text}`);\n      continue;\n    }\n    \n    allChunks.push({\n      text: chunk.text.trim(), // Normalize by trimming\n      source: chunk.source || 'research',\n      research_source: chunk.metadata?.api || 'exa',\n      rank: 0, // Will be calculated\n      quality_score: chunk.metadata?.quality_score || 0.8,\n      metadata: chunk.metadata,\n    });\n  }\n\n  // 2. Generate additional LLM chunks if needed (based on chunks plan)\n  const llmChunks = await generateLLMChunks(\n    blueprint,\n    research,\n    openai,\n    genModel,\n    costBreakdown\n  );\n\n  // 2. Add LLM-generated chunks (already validated in generateLLMChunks)\n  for (const chunk of llmChunks) {\n    // Double-check validation (should already be validated, but be safe)\n    if (!chunk || typeof chunk !== 'string' || chunk.trim().length === 0) {\n      console.warn(`[chunks] Skipping LLM chunk with invalid text`);\n      continue;\n    }\n    \n    allChunks.push({\n      text: chunk.trim(), // Ensure trimmed\n      source: 'llm_generation',\n      research_source: 'llm_generation',\n      rank: 0,\n      quality_score: 0.7, // Slightly lower than research\n    });\n  }\n\n  console.log(`[chunks] Collected ${allChunks.length} total chunks from all sources`);\n\n  // 3. Rank chunks by relevance and quality\n  const rankedChunks = await rankChunks(\n    allChunks,\n    blueprint,\n    openai,\n    genModel\n  );\n\n  // 4. Select top N chunks based on target count\n  const targetCount = blueprint.chunks_plan.target_count || 500;\n  const selectedChunks = rankedChunks.slice(0, targetCount);\n\n  console.log(`[chunks] Selected top ${selectedChunks.length} chunks after ranking`);\n\n  // 5. Generate embeddings and store in database\n  let insertedCount = 0;\n  const embeddingBatchSize = 10; // Process embeddings in batches\n\n  for (let i = 0; i < selectedChunks.length; i += embeddingBatchSize) {\n    const batch = selectedChunks.slice(i, i + embeddingBatchSize);\n\n    // Filter out chunks with invalid text\n    const validBatch = batch.filter(chunk => {\n      if (!chunk.text || typeof chunk.text !== 'string' || chunk.text.trim().length === 0) {\n        console.warn(`[chunks] Skipping chunk with invalid text (rank ${chunk.rank}): text is ${typeof chunk.text === 'string' ? 'empty' : 'not a string'}`);\n        return false;\n      }\n      return true;\n    });\n\n    if (validBatch.length === 0) {\n      console.warn(`[chunks] Batch ${i / embeddingBatchSize + 1} has no valid chunks, skipping`);\n      continue;\n    }\n\n    try {\n      // Generate embeddings in parallel\n      // Filter again and validate text is a non-empty string\n      const embeddingBatch = validBatch\n        .map(chunk => {\n          const text = typeof chunk.text === 'string' ? chunk.text.trim() : String(chunk.text || '').trim();\n          return { ...chunk, text };\n        })\n        .filter(chunk => {\n          if (!chunk.text || chunk.text.length === 0) {\n            console.warn(`[chunks] Skipping chunk with empty text after trimming (rank ${chunk.rank})`);\n            return false;\n          }\n          // OpenAI embeddings API has a maximum input length (8191 tokens for text-embedding-3-small)\n          // Rough estimate: 1 token â‰ˆ 4 characters, so max ~32k characters\n          if (chunk.text.length > 32000) {\n            console.warn(`[chunks] Skipping chunk with text too long (${chunk.text.length} chars, rank ${chunk.rank}), truncating`);\n            chunk.text = chunk.text.substring(0, 32000);\n          }\n          return true;\n        });\n\n      if (embeddingBatch.length === 0) {\n        console.warn(`[chunks] Batch ${i / embeddingBatchSize + 1} has no valid chunks after final validation, skipping`);\n        continue;\n      }\n\n      const embeddingPromises = embeddingBatch.map(chunk => {\n        // Double-check that input is a valid non-empty string\n        if (typeof chunk.text !== 'string' || chunk.text.length === 0) {\n          throw new Error(`Invalid chunk text: ${typeof chunk.text} (length: ${chunk.text?.length || 0})`);\n        }\n        return openai.embeddings.create({\n          model: embedModel,\n          input: chunk.text,\n        });\n      });\n\n      const embeddingResponses = await Promise.all(embeddingPromises);\n\n      // Track embedding costs\n      for (const embeddingResponse of embeddingResponses) {\n        if (embeddingResponse.usage) {\n          const usage = embeddingResponse.usage as Partial<OpenAIUsage>;\n          const promptTokens = usage.prompt_tokens ?? 0;\n          const completionTokens = usage.completion_tokens ?? 0;\n          const totalTokens = usage.total_tokens ?? promptTokens + completionTokens;\n          const usageForCost: OpenAIUsage = {\n            prompt_tokens: promptTokens,\n            completion_tokens: completionTokens,\n            total_tokens: totalTokens,\n          };\n          const cost = calculateOpenAICost(usageForCost, embedModel, true); // isEmbedding = true\n          costBreakdown.openai.total += cost;\n          costBreakdown.openai.embeddings.push({\n            cost,\n            usage: {\n              prompt_tokens: promptTokens,\n              completion_tokens: completionTokens,\n              total_tokens: totalTokens,\n            },\n            model: embedModel,\n          });\n        }\n      }\n\n      // Store chunks with embeddings\n      for (let j = 0; j < embeddingBatch.length; j++) {\n        const chunk = embeddingBatch[j];\n        const embeddingResponse = embeddingResponses[j];\n        \n        if (!embeddingResponse || !embeddingResponse.data || !embeddingResponse.data[0]) {\n          console.error(`[chunks] Invalid embedding response for chunk at rank ${chunk.rank}`);\n          continue;\n        }\n        \n        const embedding = embeddingResponse.data[0].embedding;\n\n        try {\n          // Determine component type\n          const componentType = chunk.research_source === 'llm_generation' \n            ? 'llm_generated' \n            : chunk.rank ? 'ranked' : 'research';\n\n          // Build metadata JSONB with all metadata fields\n          const itemMetadata = {\n            ...(chunk.metadata || {}),\n            source: chunk.source,\n            enrichment_source: chunk.research_source,\n            research_source: chunk.research_source,\n            component_type: componentType,\n            quality_score: chunk.quality_score || 0.8,\n            chunk_size: chunk.text.length,\n            enrichment_timestamp: new Date().toISOString(),\n          };\n\n          const { error: insertError }: SupabaseMutationResult = await supabase\n            .from('context_items')\n            .insert(asDbPayload({\n              event_id: eventId,\n              generation_cycle_id: generationCycleId,\n              chunk: chunk.text,\n              embedding: embedding,\n              rank: chunk.rank,\n              metadata: itemMetadata,\n            }));\n\n          if (insertError) {\n            console.error(`[chunks] Error inserting chunk at rank ${chunk.rank}: ${insertError.message}`);\n          } else {\n            insertedCount++;\n            // Update progress\n            const { error: progressError }: SupabaseMutationResult = await supabase\n              .from('generation_cycles')\n              .update(asDbPayload({ progress_current: insertedCount }))\n              .eq('id', generationCycleId);\n\n            if (progressError) {\n              console.warn(`[chunks] Failed to update progress for cycle ${generationCycleId}: ${progressError.message}`);\n            }\n          }\n        // TODO: narrow unknown -> OpenAIAPIError after upstream callsite analysis\n        } catch (error: unknown) {\n          const message = error instanceof Error ? error.message : String(error);\n          console.error(`[chunks] Error processing chunk: ${message}`);\n        }\n      }\n    // TODO: narrow unknown -> OpenAIAPIError after upstream callsite analysis\n    } catch (error: unknown) {\n      const message = error instanceof Error ? error.message : String(error);\n      console.error(`[chunks] Error processing batch: ${message}`);\n      if (typeof error === 'object' && error && 'response' in error) {\n        const response = (error as { response?: { data?: unknown } }).response;\n        if (response?.data) {\n          console.error(`[chunks] OpenAI API error details:`, JSON.stringify(response.data, null, 2));\n        }\n      }\n      // Log the first invalid chunk in the batch for debugging\n      if (validBatch.length > 0) {\n        const firstChunk = validBatch[0];\n        console.error(`[chunks] First chunk in failed batch - type: ${typeof firstChunk.text}, length: ${firstChunk.text?.length || 'N/A'}, text preview: ${String(firstChunk.text || '').substring(0, 100)}`);\n      }\n    }\n  }\n\n  // Calculate total cost and store in cycle metadata\n  const totalCost = costBreakdown.openai.total;\n  const costMetadata = {\n    cost: {\n      total: totalCost,\n      currency: 'USD',\n      breakdown: {\n        openai: {\n          total: costBreakdown.openai.total,\n          chat_completions: costBreakdown.openai.chat_completions,\n          embeddings: costBreakdown.openai.embeddings,\n        },\n      },\n      tracked_at: new Date().toISOString(),\n      pricing_version: getPricingVersion(),\n    },\n  };\n\n  // Mark cycle as completed with cost metadata\n  const { error: cycleUpdateError }: SupabaseMutationResult = await supabase\n    .from('generation_cycles')\n    .update(asDbPayload({\n      status: 'completed',\n      progress_current: insertedCount,\n      completed_at: new Date().toISOString(),\n      metadata: costMetadata,\n    }))\n    .eq('id', generationCycleId);\n\n  if (cycleUpdateError) {\n    console.error(`[chunks] ERROR: Failed to update generation cycle to completed: ${cycleUpdateError.message}`);\n    throw new Error(`Failed to update generation cycle: ${cycleUpdateError.message}`);\n  }\n\n  console.log(`[chunks] Inserted ${insertedCount} context chunks for event ${eventId} (cost: $${totalCost.toFixed(4)})`);\n  console.log(`[chunks] Generation cycle ${generationCycleId} marked as completed`);\n  return {\n    chunkCount: insertedCount,\n    costBreakdown,\n  };\n}\n\n/**\n * Generate additional LLM chunks based on blueprint plan\n */\nasync function generateLLMChunks(\n  blueprint: Blueprint,\n  researchResults: ResearchResults,\n  openai: OpenAI,\n  genModel: string,\n  costBreakdown: ChunksCostBreakdown\n): Promise<string[]> {\n  // Calculate how many LLM chunks we need\n  const targetCount = blueprint.chunks_plan.target_count || 500;\n  const researchChunkCount = researchResults.chunks.length;\n  const neededLLMChunks = Math.max(0, targetCount - researchChunkCount);\n\n  if (neededLLMChunks === 0) {\n    console.log(`[chunks] Research results sufficient, skipping LLM chunk generation`);\n    return [];\n  }\n\n  console.log(`[chunks] Generating ${neededLLMChunks} additional LLM chunks`);\n\n  const systemPrompt = CONTEXT_CHUNKS_GENERATION_SYSTEM_PROMPT;\n\n  const researchSummary = researchResults.chunks\n    .map((chunk: ResearchChunk) => chunk.text)\n    .join('\\n\\n')\n    .substring(0, 3000);\n\n  const blueprintDetails = [\n    `Target chunks: ${neededLLMChunks}`,\n    `Quality tier: ${blueprint.chunks_plan.quality_tier}`,\n    `Inferred topics: ${blueprint.inferred_topics.join(', ')}`\n  ].join('\\n');\n\n  const glossaryHighlights = blueprint.key_terms.slice(0, 10).join(', ');\n\n  const userPrompt = createContextChunksUserPrompt(\n    researchSummary,\n    blueprintDetails,\n    glossaryHighlights\n  );\n\n  try {\n    // Some models (like o1, o1-preview, o1-mini, gpt-5) don't support custom temperature values\n    // Only set temperature if model supports custom values\n    const isO1Model = genModel.startsWith('o1');\n    const onlySupportsDefaultTemp = isO1Model || genModel.includes('gpt-5');\n    const supportsCustomTemperature = !onlySupportsDefaultTemp;\n    \n    // Build request options - conditionally include temperature\n    const requestOptions: ChatCompletionRequest = {\n      model: genModel,\n      messages: [\n        { role: 'system', content: systemPrompt },\n        { role: 'user', content: userPrompt },\n      ],\n      response_format: { type: 'json_object' },\n    };\n    \n    // Only add temperature if model supports custom temperature values\n    if (supportsCustomTemperature) {\n      requestOptions.temperature = 0.7;\n    }\n    \n    const response = await openai.chat.completions.create(\n      requestOptions\n    ) as OpenAI.Chat.Completions.ChatCompletion;\n\n    // Track OpenAI cost for chat completion\n  if (response.usage) {\n    const usage = response.usage as Partial<OpenAIUsage>;\n    const promptTokens = usage.prompt_tokens ?? 0;\n    const completionTokens = usage.completion_tokens ?? 0;\n    const totalTokens = usage.total_tokens ?? promptTokens + completionTokens;\n    const usageForCost: OpenAIUsage = {\n      prompt_tokens: promptTokens,\n      completion_tokens: completionTokens,\n      total_tokens: totalTokens,\n    };\n    const cost = calculateOpenAICost(usageForCost, genModel, false);\n    costBreakdown.openai.total += cost;\n    costBreakdown.openai.chat_completions.push({\n      cost,\n      usage: {\n        prompt_tokens: promptTokens,\n        completion_tokens: completionTokens,\n        total_tokens: totalTokens,\n      },\n      model: genModel,\n      });\n    }\n\n    const content = response.choices[0]?.message?.content;\n    if (!content) {\n      throw new Error('Empty response from LLM');\n    }\n\n    let parsed: unknown;\n    try {\n      parsed = JSON.parse(content);\n    // TODO: narrow unknown -> SyntaxError after upstream callsite analysis\n    } catch (parseError: unknown) {\n      console.error(`[chunks] Failed to parse LLM response as JSON: ${content.substring(0, 200)}`);\n      throw new Error('LLM response is not valid JSON');\n    }\n\n    // Handle both formats: { chunks: [...] } and [...] (array directly)\n    let chunks: unknown[] = [];\n    if (Array.isArray(parsed)) {\n      chunks = parsed;\n    } else if (parsed && Array.isArray((parsed as { chunks?: unknown[] }).chunks)) {\n      chunks = (parsed as { chunks: unknown[] }).chunks ?? [];\n    } else {\n      console.error(`[chunks] Unexpected LLM response format. Parsed:`, JSON.stringify(parsed).substring(0, 200));\n      throw new Error('LLM did not return array of chunks in expected format');\n    }\n\n    // Filter and validate chunks - ensure they are non-empty strings\n    const validChunks = chunks\n      .filter((chunk): chunk is string => typeof chunk === 'string' && chunk.trim().length > 0)\n      .map(chunk => chunk.trim()) // Normalize by trimming\n      .slice(0, neededLLMChunks);\n\n    console.log(`[chunks] Generated ${validChunks.length} valid LLM chunks (filtered ${chunks.length - validChunks.length} invalid)`);\n\n    return validChunks;\n  // TODO: narrow unknown -> OpenAIAPIError after upstream callsite analysis\n  } catch (error: unknown) {\n    const message = error instanceof Error ? error.message : String(error);\n    console.error(`[chunks] Error generating LLM chunks: ${message}`);\n    return [];\n  }\n}\n\n/**\n * Rank chunks by relevance and quality\n */\nasync function rankChunks(\n  chunks: ChunkWithRank[],\n  blueprint: Blueprint,\n  openai: OpenAI,\n  genModel: string\n): Promise<ChunkWithRank[]> {\n  // Simple ranking strategy: combine quality score with source priority\n  // Research results get higher priority, then LLM generation\n  const sourcePriority: Record<string, number> = {\n    'exa': 1.0,\n    'wikipedia': 0.9,\n    'llm_generation': 0.7,\n    'research': 0.8,\n  };\n\n  // Calculate scores for each chunk\n  const scoredChunks = chunks.map((chunk, index) => {\n    const sourceScore = sourcePriority[chunk.research_source] || 0.5;\n    const qualityScore = chunk.quality_score || 0.7;\n    const combinedScore = sourceScore * 0.6 + qualityScore * 0.4;\n\n    return {\n      ...chunk,\n      _score: combinedScore,\n    };\n  });\n\n  // Sort by score (descending) and assign ranks\n  scoredChunks.sort((a, b) => b._score - a._score);\n\n  return scoredChunks.map((chunk, index) => ({\n    ...chunk,\n    rank: index + 1, // Rank 1 = highest\n    _score: undefined, // Remove temporary score\n  } as ChunkWithRank));\n}\n","usedDeprecatedRules":[]}]