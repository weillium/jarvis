[{"filePath":"/Users/will-liao/Desktop/Coding/Git/jarvis/worker/context/pipeline/glossary-builder.ts","messages":[{"ruleId":"@typescript-eslint/no-unused-vars","severity":2,"message":"'ErrorWithResponse' is defined but never used.","line":85,"column":6,"nodeType":null,"messageId":"unusedVar","endLine":85,"endColumn":23}],"suppressedMessages":[],"errorCount":1,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"source":"/**\n * Glossary Builder\n * Builds glossary from blueprint plan and research results\n * Stores terms, definitions, acronyms, and related metadata in glossary_terms table\n */\n\nimport type { createClient } from '@supabase/supabase-js';\nimport type OpenAI from 'openai';\nimport { Exa } from 'exa-js';\nimport type { Blueprint } from './blueprint-generator';\nimport {\n  calculateOpenAICost,\n  calculateExaAnswerCost,\n  getPricingVersion,\n  type OpenAIUsage,\n} from './pricing-config';\nimport {\n  EXA_ANSWER_SYSTEM_PROMPT,\n  GLOSSARY_DEFINITION_SYSTEM_PROMPT,\n  createGlossaryDefinitionUserPrompt,\n  EXA_ANSWER_TRANSFORM_SYSTEM_PROMPT,\n  createExaAnswerTransformUserPrompt,\n} from '../../prompts';\n\nexport interface GlossaryBuilderOptions {\n  supabase: ReturnType<typeof createClient>;\n  openai: OpenAI;\n  genModel: string;\n  embedModel: string;\n  exaApiKey?: string; // Optional Exa API key for authoritative definitions\n}\n\nexport type ResearchChunkMetadata = {\n  api?: string;\n  quality_score?: number;\n} & Record<string, unknown>;\n\nexport interface ResearchResults {\n  chunks: Array<{\n    text: string;\n    source: string;\n    metadata?: ResearchChunkMetadata;\n  }>;\n}\ntype ResearchChunk = ResearchResults['chunks'][number];\n\n/**\n * Build glossary from blueprint plan and research results\n * Fetches research from research_results table if not provided\n */\nexport interface GlossaryCostBreakdown {\n  openai: {\n    total: number;\n    chat_completions: Array<{ cost: number; usage: OpenAIUsage; model: string }>;\n  };\n  exa: {\n    total: number;\n    answer: { cost: number; queries: number };\n  };\n}\n\nexport interface GlossaryBuildResult {\n  termCount: number;\n  costBreakdown: GlossaryCostBreakdown;\n}\n\ntype SupabaseErrorLike = { message: string } | null;\ntype SupabaseMutationResult = { error: SupabaseErrorLike };\ntype SupabaseListResult<T> = { data: T[] | null; error: SupabaseErrorLike };\ntype IdRow = { id: string };\ntype ResearchResultRecord = {\n  content: string;\n  metadata: ResearchChunkMetadata | null;\n  query: string | null;\n  api: string | null;\n};\ntype ChatCompletionRequest = Parameters<OpenAI['chat']['completions']['create']>[0];\ntype GlossaryPlanTerm = {\n  term: string;\n  is_acronym: boolean;\n  category: string;\n  priority: number;\n};\nconst asDbPayload = <T>(payload: T) => payload as unknown as never;\ntype ErrorWithResponse = Error & { response?: { data?: unknown } };\n\nexport async function buildGlossary(\n  eventId: string,\n  blueprintId: string,\n  generationCycleId: string,\n  blueprint: Blueprint,\n  researchResults: ResearchResults | null,\n  options: GlossaryBuilderOptions\n): Promise<GlossaryBuildResult> {\n  const { supabase, openai, genModel, exaApiKey } = options;\n\n  console.log(`[glossary] Building glossary for event ${eventId}, cycle ${generationCycleId}`);\n  console.log(`[glossary] Blueprint has ${blueprint.glossary_plan.terms.length} terms planned`);\n\n  const termsToBuild = blueprint.glossary_plan.terms || [];\n  if (termsToBuild.length === 0) {\n    console.log(`[glossary] No terms to build, skipping`);\n    return {\n      termCount: 0,\n      costBreakdown: {\n        openai: { total: 0, chat_completions: [] },\n        exa: { total: 0, answer: { cost: 0, queries: 0 } },\n      },\n    };\n  }\n\n  // Initialize cost tracking\n  const costBreakdown: GlossaryCostBreakdown = {\n    openai: {\n      total: 0,\n      chat_completions: [],\n    },\n    exa: {\n      total: 0,\n      answer: { cost: 0, queries: 0 },\n    },\n  };\n\n  // Fetch research from research_results table if not provided\n  // Exclude research from superseded generation cycles\n  let research: ResearchResults;\n  if (!researchResults) {\n    // First, get all active (non-superseded) generation cycle IDs for research\n    const {\n      data: activeCycles,\n      error: cycleError,\n    }: SupabaseListResult<IdRow> = await supabase\n      .from('generation_cycles')\n      .select('id')\n      .eq('event_id', eventId)\n      .neq('status', 'superseded')\n      .in('cycle_type', ['research']);\n\n    if (cycleError) {\n      console.warn(`[glossary] Warning: Failed to fetch active research cycles: ${cycleError.message}`);\n    }\n\n    // Build list of active cycle IDs\n    const activeCycleIds: string[] = [];\n    if (activeCycles && activeCycles.length > 0) {\n      activeCycleIds.push(...activeCycles.map((cycle: IdRow) => cycle.id));\n    }\n\n    // Fetch research results only from active cycles (or legacy items)\n    let researchQuery = supabase\n      .from('research_results')\n      .select('content, metadata, query, api')\n      .eq('event_id', eventId)\n      .eq('blueprint_id', blueprintId);\n\n    if (activeCycleIds.length > 0) {\n      // Include items with null generation_cycle_id OR items from active cycles\n      researchQuery = researchQuery.or(`generation_cycle_id.is.null,generation_cycle_id.in.(${activeCycleIds.join(',')})`);\n    } else {\n      // If no active cycles, only show legacy items (null generation_cycle_id)\n      researchQuery = researchQuery.is('generation_cycle_id', null);\n    }\n\n    const {\n      data: researchData,\n      error: researchError,\n    }: SupabaseListResult<ResearchResultRecord> = await researchQuery;\n\n    if (researchError) {\n      console.warn(`[glossary] Warning: Failed to fetch research results: ${researchError.message}`);\n    }\n\n    research = {\n      chunks: (researchData ?? []).map((item: ResearchResultRecord) => ({\n        text: item.content,\n        source: item.api || 'research',\n        metadata: item.metadata || undefined,\n      })),\n    };\n  } else {\n    research = researchResults;\n  }\n\n  // Legacy deletion code removed - we now use superseding approach\n  // Old glossary terms are marked as superseded via generation cycles, not deleted\n\n  // Extract context from research results\n  const researchContext = research.chunks\n    .map((chunk: ResearchChunk) => chunk.text)\n    .join('\\n\\n')\n    .substring(0, 10000); // Limit context size\n\n  let insertedCount = 0;\n\n  // Update generation cycle progress\n  const { error: processingError }: SupabaseMutationResult = await supabase\n    .from('generation_cycles')\n    .update(asDbPayload({\n      status: 'processing',\n      progress_total: termsToBuild.length,\n    }))\n    .eq('id', generationCycleId);\n\n  if (processingError) {\n    console.warn(`[glossary] Failed to mark cycle ${generationCycleId} as processing: ${processingError.message}`);\n  }\n\n  // Process terms in batches to avoid rate limits\n  const batchSize = 5;\n  for (let i = 0; i < termsToBuild.length; i += batchSize) {\n    const batch = termsToBuild.slice(i, i + batchSize);\n    \n    try {\n      const { definitions } = await generateTermDefinitions(\n        batch,\n        researchContext,\n        blueprint.important_details.join('\\n'),\n        openai,\n        genModel,\n        exaApiKey ? new Exa(exaApiKey) : undefined,\n        costBreakdown\n      );\n\n      // Store definitions in database\n      for (const def of definitions) {\n        try {\n          const { error: insertError }: SupabaseMutationResult = await supabase\n            .from('glossary_terms')\n            .insert(asDbPayload({\n              event_id: eventId,\n              generation_cycle_id: generationCycleId,\n              term: def.term,\n              definition: def.definition,\n              acronym_for: def.acronym_for || null,\n              category: def.category || 'general',\n              usage_examples: def.usage_examples || [],\n              related_terms: def.related_terms || [],\n              confidence_score: def.confidence_score || 0.8,\n              source: def.source || 'llm_generation',\n              source_url: def.source_url || null,\n            }));\n\n          if (insertError) {\n            console.error(`[glossary] Error inserting term \"${def.term}\": ${insertError.message}`);\n          } else {\n            insertedCount++;\n            // Update progress\n            const { error: progressError }: SupabaseMutationResult = await supabase\n              .from('generation_cycles')\n              .update(asDbPayload({ progress_current: insertedCount }))\n              .eq('id', generationCycleId);\n\n            if (progressError) {\n              console.warn(`[glossary] Failed to update cycle progress for ${generationCycleId}: ${progressError.message}`);\n            }\n          }\n        // TODO: narrow unknown -> PostgrestError after upstream callsite analysis\n        } catch (error: unknown) {\n          const message = error instanceof Error ? error.message : String(error);\n          console.error(`[glossary] Error processing term \"${def.term}\": ${message}`);\n        }\n      }\n    // TODO: narrow unknown -> OpenAIAPIError after upstream callsite analysis\n    } catch (error: unknown) {\n      const message = error instanceof Error ? error.message : String(error);\n      console.error(`[glossary] Error processing batch: ${message}`);\n    }\n  }\n\n  // Calculate total cost and store in cycle metadata\n  const totalCost = costBreakdown.openai.total + costBreakdown.exa.total;\n  const costMetadata = {\n    cost: {\n      total: totalCost,\n      currency: 'USD',\n      breakdown: {\n        openai: {\n          total: costBreakdown.openai.total,\n          chat_completions: costBreakdown.openai.chat_completions,\n        },\n        exa: {\n          total: costBreakdown.exa.total,\n          answer: costBreakdown.exa.answer,\n        },\n      },\n      tracked_at: new Date().toISOString(),\n      pricing_version: getPricingVersion(),\n    },\n  };\n\n  // Mark cycle as completed with cost metadata\n  const { error: cycleUpdateError }: SupabaseMutationResult = await supabase\n    .from('generation_cycles')\n    .update(asDbPayload({\n      status: 'completed',\n      progress_current: insertedCount,\n      completed_at: new Date().toISOString(),\n      metadata: costMetadata,\n    }))\n    .eq('id', generationCycleId);\n\n  if (cycleUpdateError) {\n    console.error(`[glossary] ERROR: Failed to update generation cycle to completed: ${cycleUpdateError.message}`);\n    throw new Error(`Failed to update generation cycle: ${cycleUpdateError.message}`);\n  }\n\n  console.log(`[glossary] Inserted ${insertedCount} glossary terms for event ${eventId}`);\n  console.log(`[glossary] Generation cycle ${generationCycleId} marked as completed`);\n  return {\n    termCount: insertedCount,\n    costBreakdown,\n  };\n}\n\ninterface TermDefinition {\n  term: string;\n  definition: string;\n  acronym_for?: string;\n  category: string;\n  usage_examples?: string[];\n  related_terms?: string[];\n  confidence_score?: number;\n  source?: string;\n  source_url?: string;\n}\n\n/**\n * Generate definitions for terms using Exa /answer for high-priority terms, LLM for others\n */\nasync function generateTermDefinitions(\n  terms: Array<{ term: string; is_acronym: boolean; category: string; priority: number }>,\n  researchContext: string,\n  importantDetails: string,\n  openai: OpenAI,\n  genModel: string,\n  exa?: Exa,\n  costBreakdown?: GlossaryCostBreakdown\n): Promise<{ definitions: TermDefinition[]; batchCostBreakdown: GlossaryCostBreakdown }> {\n  const definitions: TermDefinition[] = [];\n  const termsForLLM: Array<{ term: string; is_acronym: boolean; category: string; priority: number }> = [];\n  \n  // Initialize batch cost breakdown\n  const batchCostBreakdown: GlossaryCostBreakdown = {\n    openai: {\n      total: 0,\n      chat_completions: [],\n    },\n    exa: {\n      total: 0,\n      answer: { cost: 0, queries: 0 },\n    },\n  };\n\n  // Process high-priority terms (priority <= 3) with Exa /answer if available\n  for (const term of terms) {\n    if (term.priority <= 3 && exa) {\n      try {\n        console.log(`[glossary] Using Exa /answer for high-priority term (priority ${term.priority}): ${term.term}`);\n        \n        const answer = await exa.answer(`What is ${term.term}?`, {\n          text: true,\n          systemPrompt: EXA_ANSWER_SYSTEM_PROMPT,\n        });\n\n        const answerText = typeof answer.answer === 'string' ? answer.answer.trim() : '';\n        if (answerText) {\n          // Extract source URL from citations if available\n          const sourceUrl = Array.isArray(answer.citations) && answer.citations.length > 0\n            ? answer.citations[0]?.url \n            : undefined;\n\n          // Transform Exa markdown answer into structured glossary format using LLM\n          const transformedDef = await transformExaAnswerToGlossary(\n            term.term,\n            term.is_acronym,\n            term.category,\n            answerText,\n            sourceUrl,\n            openai,\n            genModel\n          );\n\n          if (transformedDef) {\n            definitions.push(transformedDef);\n            console.log(`[glossary] Generated definition for \"${term.term}\" using Exa /answer (transformed to glossary format)`);\n            \n            // Track Exa answer cost\n            const answerCost = calculateExaAnswerCost(1);\n            batchCostBreakdown.exa.total += answerCost;\n            batchCostBreakdown.exa.answer.cost += answerCost;\n            batchCostBreakdown.exa.answer.queries += 1;\n            \n            continue; // Skip LLM generation for this term\n          } else {\n            console.warn(`[glossary] Failed to transform Exa answer for \"${term.term}\", falling back to LLM`);\n            // Fall through to LLM generation\n          }\n        }\n      // TODO: narrow unknown -> ExaAPIError after upstream callsite analysis\n      } catch (exaError: unknown) {\n        const message = exaError instanceof Error ? exaError.message : String(exaError);\n        console.warn(`[glossary] Exa /answer failed for term \"${term.term}\": ${message}. Falling back to LLM.`);\n        // Fall through to LLM generation\n      }\n    }\n\n    // Add to LLM batch if Exa wasn't used or failed\n    termsForLLM.push(term);\n  }\n\n  // Generate remaining terms with LLM\n  if (termsForLLM.length > 0) {\n    const systemPrompt = GLOSSARY_DEFINITION_SYSTEM_PROMPT;\n\n    const termsList = termsForLLM\n      .map((term: GlossaryPlanTerm) => `- ${term.term}${term.is_acronym ? ' (acronym)' : ''} - ${term.category}`)\n      .join('\\n');\n\n    const userPrompt = createGlossaryDefinitionUserPrompt(\n      termsList,\n      researchContext,\n      importantDetails\n    );\n\n    try {\n      // Some models (like o1, o1-preview, o1-mini, gpt-5) don't support custom temperature values\n      // Only set temperature if model supports custom values\n      // Check for models that only support default temperature (1) or don't support it at all\n      const modelLower = genModel.toLowerCase();\n      const isO1Model = modelLower.startsWith('o1');\n      const isGpt5Model = modelLower.includes('gpt-5') || modelLower.startsWith('gpt5');\n      const onlySupportsDefaultTemp = isO1Model || isGpt5Model;\n      const supportsCustomTemperature = !onlySupportsDefaultTemp;\n      \n      if (onlySupportsDefaultTemp) {\n        console.log(`[glossary] Model \"${genModel}\" only supports default temperature (1), skipping custom temperature setting`);\n      }\n      \n      // Build request options - conditionally include temperature\n      const requestOptions: ChatCompletionRequest = {\n        model: genModel,\n        messages: [\n          { role: 'system', content: systemPrompt },\n          { role: 'user', content: userPrompt },\n        ],\n        response_format: { type: 'json_object' },\n      };\n      \n      // Only add temperature if model supports custom temperature values\n      if (supportsCustomTemperature) {\n        requestOptions.temperature = 0.5; // Lower temperature for more consistent definitions\n      }\n      \n      const response = await openai.chat.completions.create(\n        requestOptions\n      ) as OpenAI.Chat.Completions.ChatCompletion;\n\n      // Track OpenAI cost\n      if (response.usage) {\n        const usage = response.usage;\n        const cost = calculateOpenAICost(usage, genModel, false);\n        batchCostBreakdown.openai.total += cost;\n        batchCostBreakdown.openai.chat_completions.push({\n          cost,\n          usage: {\n            prompt_tokens: usage.prompt_tokens,\n            completion_tokens: usage.completion_tokens,\n            total_tokens: usage.total_tokens,\n          },\n          model: genModel,\n        });\n      }\n\n      const content = response.choices[0]?.message?.content;\n      if (!content) {\n        throw new Error('Empty response from LLM');\n      }\n\n      const parsed: unknown = JSON.parse(content);\n      // Handle both \"definitions\" and \"terms\" keys (json_object format always returns object)\n      const llmDefinitions =\n        (parsed as { definitions?: unknown; terms?: unknown }).definitions ??\n        (parsed as { definitions?: unknown; terms?: unknown }).terms ??\n        [];\n\n      if (!Array.isArray(llmDefinitions)) {\n        throw new Error('LLM did not return array of definitions');\n      }\n\n      // Validate and normalize definitions\n      const normalizedLLMDefinitions = llmDefinitions.map((def: unknown) => {\n        const normalized = def as Partial<TermDefinition>;\n        return {\n          term: normalized.term || '',\n          definition: normalized.definition || '',\n          acronym_for: normalized.acronym_for || undefined,\n          category: normalized.category || 'general',\n          usage_examples: normalized.usage_examples || [],\n          related_terms: normalized.related_terms || [],\n          confidence_score: normalized.confidence_score || 0.8,\n          source: normalized.source || 'llm_generation',\n          source_url: normalized.source_url || undefined,\n        };\n      });\n\n      definitions.push(...normalizedLLMDefinitions);\n    // TODO: narrow unknown -> OpenAIAPIError after upstream callsite analysis\n    } catch (error: unknown) {\n      const message = error instanceof Error ? error.message : String(error);\n      console.error(`[glossary] Error generating LLM definitions: ${message}`);\n      // Return basic definitions on error\n      const fallbackDefinitions = termsForLLM.map((term: GlossaryPlanTerm) => ({\n        term: term.term,\n        definition: `Term: ${term.term}. Definition to be completed.`,\n        category: term.category,\n        confidence_score: 0.5,\n        source: 'llm_generation',\n      }));\n      definitions.push(...fallbackDefinitions);\n    }\n  }\n\n  // Merge batch costs into main cost breakdown if provided\n  if (costBreakdown) {\n    costBreakdown.openai.total += batchCostBreakdown.openai.total;\n    costBreakdown.openai.chat_completions.push(...batchCostBreakdown.openai.chat_completions);\n    costBreakdown.exa.total += batchCostBreakdown.exa.total;\n    costBreakdown.exa.answer.cost += batchCostBreakdown.exa.answer.cost;\n    costBreakdown.exa.answer.queries += batchCostBreakdown.exa.answer.queries;\n  }\n\n  return { definitions, batchCostBreakdown };\n}\n\n/**\n * Transform Exa markdown answer into structured glossary format\n * Extracts clean definition, usage examples, and related terms\n */\nasync function transformExaAnswerToGlossary(\n  term: string,\n  isAcronym: boolean,\n  category: string,\n  exaAnswer: string,\n  sourceUrl: string | undefined,\n  openai: OpenAI,\n  genModel: string\n): Promise<TermDefinition | null> {\n  try {\n    const systemPrompt = EXA_ANSWER_TRANSFORM_SYSTEM_PROMPT;\n    const termDescriptor = `- ${term}${isAcronym ? ' (acronym)' : ''} - ${category}`;\n    const userPrompt = createExaAnswerTransformUserPrompt(termDescriptor, exaAnswer);\n\n    // Some models (like o1, o1-preview, o1-mini, gpt-5) don't support custom temperature values\n    const modelLower = genModel.toLowerCase();\n    const isO1Model = modelLower.startsWith('o1');\n    const isGpt5Model = modelLower.includes('gpt-5') || modelLower.startsWith('gpt5');\n    const onlySupportsDefaultTemp = isO1Model || isGpt5Model;\n    const supportsCustomTemperature = !onlySupportsDefaultTemp;\n\n    const requestOptions: ChatCompletionRequest = {\n      model: genModel,\n      messages: [\n        { role: 'system', content: systemPrompt },\n        { role: 'user', content: userPrompt },\n      ],\n      response_format: { type: 'json_object' },\n    };\n\n    if (supportsCustomTemperature) {\n      requestOptions.temperature = 0.3; // Low temperature for consistent transformation\n    }\n\n    const response = await openai.chat.completions.create(\n      requestOptions\n    ) as OpenAI.Chat.Completions.ChatCompletion;\n    const content = response.choices[0]?.message?.content;\n\n    if (!content) {\n      console.warn(`[glossary] Empty response when transforming Exa answer for \"${term}\"`);\n      return null;\n    }\n\n    try {\n      const parsed = JSON.parse(content) as {\n        term?: string;\n        definition?: string;\n        acronym_for?: string;\n        category?: string;\n        usage_examples?: string[];\n        related_terms?: string[];\n      };\n\n      if (!parsed.definition || !parsed.definition.trim()) {\n        console.warn(`[glossary] Missing definition in transformed Exa answer for \"${term}\"`);\n        return null;\n      }\n\n      return {\n        term: parsed.term || term,\n        definition: parsed.definition.trim(),\n        acronym_for: parsed.acronym_for || undefined,\n        category: parsed.category || category,\n        usage_examples: parsed.usage_examples || [],\n        related_terms: parsed.related_terms || [],\n        confidence_score: 0.9, // High confidence for Exa answers\n        source: 'exa',\n        source_url: sourceUrl,\n      };\n    // TODO: narrow unknown -> SyntaxError after upstream callsite analysis\n    } catch (parseError: unknown) {\n      const message = parseError instanceof Error ? parseError.message : String(parseError);\n      console.warn(`[glossary] Failed to parse transformed Exa answer for \"${term}\": ${message}`);\n      return null;\n    }\n  // TODO: narrow unknown -> OpenAIAPIError after upstream callsite analysis\n  } catch (error: unknown) {\n    const message = error instanceof Error ? error.message : String(error);\n    console.warn(`[glossary] Error transforming Exa answer for \"${term}\": ${message}`);\n    return null;\n  }\n}\n","usedDeprecatedRules":[]}]